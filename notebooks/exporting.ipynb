{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting ü§ó Hugging Face models to ONNX\n",
    "You can use both Python modules used as a command-line tool or use the libraries directly. This notebook will explore how to export using the `transformers.onnx` module as a CLI and then consume that resulting model with Hugging Face and the `onnxruntime` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the features\n",
    "List the available features for a specific model. You can then use that feature when exporting to ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['default', 'masked-lm', 'sequence-classification', 'multiple-choice', 'token-classification', 'question-answering']\n"
     ]
    }
   ],
   "source": [
    "from transformers.onnx.features import FeaturesManager\n",
    "\n",
    "distilbert_features = list(FeaturesManager.get_supported_features_for_model_type(\"distilbert\").keys())\n",
    "print(distilbert_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx\n",
      "  Obtaining dependency information for onnx from https://files.pythonhosted.org/packages/95/ed/84689505ed7b73cf70f72bc6d7e978d608623f60b2d4efafdef425b2f347/onnx-1.14.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading onnx-1.14.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from onnx) (1.24.3)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /opt/conda/lib/python3.8/site-packages (from onnx) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /opt/conda/lib/python3.8/site-packages (from onnx) (4.7.1)\n",
      "Downloading onnx-1.14.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: onnx\n",
      "Successfully installed onnx-1.14.1\n"
     ]
    }
   ],
   "source": [
    "! pip install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Framework not requested. Using torch to export to ONNX.\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using framework PyTorch: 2.0.1\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:223: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Validating ONNX model...\n",
      "\t-[‚úì] ONNX model output names match reference model ({'start_logits', 'end_logits'})\n",
      "\t- Validating ONNX Model output \"start_logits\":\n",
      "\t\t-[‚úì] (3, 9) matches (3, 9)\n",
      "\t\t-[‚úì] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"end_logits\":\n",
      "\t\t-[‚úì] (3, 9) matches (3, 9)\n",
      "\t\t-[‚úì] all values close (atol: 1e-05)\n",
      "All good, model saved at: model.onnx\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/onnx/__main__.py:180: FutureWarning: The export was done by transformers.onnx which is deprecated and will be removed in v5. We recommend using optimum.exporters.onnx in future. You can find more information here: https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!python -m transformers.onnx --model=distilbert-base-uncased-finetuned-sst-2-english \\\n",
    "                            --feature=question-answering ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore and further reading\n",
    "Go through the ONNX models and how they can work with the `onnxruntime` in the [ONNX Zoo](https://github.com/onnx/models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
