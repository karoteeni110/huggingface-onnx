{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤— Hugging Face and ONNX introduction\n",
    "ONNX Runtime is a performance-focused inference engine for ONNX models. It is cross-platform and open source. It supports execution of models on a wide range of devices, from CPUs to GPUs.\n",
    "\n",
    "ONNX allows for interoperability between different frameworks and tools, it is open source and therefore can be easily extended, and it is efficient and scalable.\n",
    "\n",
    "This notebook will explore the ONNX runtime and how to use it with Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation \n",
    "Make sure you are using the right libraries. It might be easier to use conda to install the dependencies and runtime settings. For example:\n",
    "\n",
    "```yaml\n",
    "name: huggingface-onnx\n",
    "\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - pytorch::pytorch\n",
    "  - pip\n",
    "  - transformers[onnx]\n",
    "  - pip:\n",
    "    - ipywidgets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported with ready-made configurations\n",
    "There are a few models that are supported right away with configurations, which means that it will be easy to port an existing Hugging Face model over to ONNX.\n",
    "\n",
    "Some of the popular models are:\n",
    "\n",
    "- BART\n",
    "- BERT\n",
    "- Data2VecText\n",
    "- Data2VecVision\n",
    "- DistilBERT\n",
    "- OpenAI GPT-2\n",
    "- RoBERTa\n",
    "- T5\n",
    "- Whisper\n",
    "- YOLOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: Hugging Face Transformers ONNX exporter [-h] -m MODEL\n",
      "                                               [--feature FEATURE]\n",
      "                                               [--opset OPSET] [--atol ATOL]\n",
      "                                               [--framework {pt,tf}]\n",
      "                                               [--cache_dir CACHE_DIR]\n",
      "                                               [--preprocessor {auto,tokenizer,feature_extractor,image_processor,processor}]\n",
      "                                               [--export_with_transformers]\n",
      "                                               output\n",
      "\n",
      "positional arguments:\n",
      "  output                Path indicating where to store generated ONNX model.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -m MODEL, --model MODEL\n",
      "                        Model ID on huggingface.co or path on disk to load\n",
      "                        model from.\n",
      "  --feature FEATURE     The type of features to export the model with.\n",
      "  --opset OPSET         ONNX opset version to export the model with.\n",
      "  --atol ATOL           Absolute difference tolerance when validating the\n",
      "                        model.\n",
      "  --framework {pt,tf}   The framework to use for the ONNX export. If not\n",
      "                        provided, will attempt to use the local checkpoint's\n",
      "                        original framework or what is available in the\n",
      "                        environment.\n",
      "  --cache_dir CACHE_DIR\n",
      "                        Path indicating where to store cache.\n",
      "  --preprocessor {auto,tokenizer,feature_extractor,image_processor,processor}\n",
      "                        Which type of preprocessor to use. 'auto' tries to\n",
      "                        automatically detect it.\n",
      "  --export_with_transformers\n",
      "                        Whether to use transformers.onnx instead of\n",
      "                        optimum.exporters.onnx to perform the ONNX export. It\n",
      "                        can be useful when exporting a model supported in\n",
      "                        transformers but not in optimum, otherwise it is not\n",
      "                        recommended.\n"
     ]
    }
   ],
   "source": [
    "!python -m transformers.onnx --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs\n",
    "Configurations are important because they allow you correctly interact and map with the resulting model. Hugging Face already includes ready-to-use configurations for the popular models, so no need to figure out how that mapping should go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.roberta import RobertaConfig, RobertaOnnxConfig\n",
    "config = RobertaConfig()\n",
    "onnx_config = RobertaOnnxConfig(config)\n",
    "print(list(onnx_config.inputs.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested reading\n",
    "Learn more about some other options and other supported serialization to ONNX in the [Hugging Face Transformers Serialization](https://huggingface.co/docs/transformers/serialization) documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
